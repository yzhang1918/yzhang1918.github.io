<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>图卷积GCN的直观理解 - Ylog</title><link rel="icon" type="image/png" href=icons/Y.png /><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="图卷积GCN的直观理解" />
<meta property="og:description" content="旧文搬运" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yzhang1918.github.io/posts/gcn_intuition/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-17T20:51:33&#43;08:00" />
<meta property="article:modified_time" content="2020-02-17T20:51:33&#43;08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="图卷积GCN的直观理解"/>
<meta name="twitter:description" content="旧文搬运"/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://yzhang1918.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://yzhang1918.github.io/css/main.css" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
	<script src="https://yzhang1918.github.io/js/main.js"></script>
</head>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '\\[', right: '\\]', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false}
                ]
            });"></script>
        

<body>
	<div class="container wrapper post">
		<div class="header">
	<h1 class="site-title"><a href="https://yzhang1918.github.io/">Ylog</a></h1>
	<div class="site-description"><h2>yzhang1918&rsquo;s Blog</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/yzhang1918" title="Github"><i data-feather="github"></i></a><a href="https://www.instagram.com/yaozh1918/" title="Instagram"><i data-feather="instagram"></i></a><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/posts">All posts</a>
			</li>
			
			<li>
				<a href="/categories">Categories</a>
			</li>
			
			<li>
				<a href="/tags">Tags</a>
			</li>
			
			<li>
				<a href="/about">About</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">图卷积GCN的直观理解</h1>
			<div class="meta">Posted at &mdash; Feb 17, 2020</div>
		</div>

		<div class="post-tags">
			
				
				<nav class="nav">
						<ul class="flat">
							
							<li class="tags"><a href="/tags/gnn">GNN</a></li>
							
							
							<li class="categories"><a href="/categories/research">research</a></li>
							
							<li class="categories"><a href="/categories/tutorial">tutorial</a></li>
							
						</ul>
				</nav>
				
			
		</div>

		<div class="markdown">
			<p>实验室又有新生进来了，也有对图卷积感兴趣的，那就写篇文章，简单的讲一下。本篇文章参考了<a href="https://tkipf.github.io/graph-convolutional-networks/">Kipf的博客</a>，主要是直观的理解，加上我的一点解读，数学上的推导咱们以后有空再说。</p>
<div class="toc">
<nav id="TableOfContents">
  <ul>
    <li><a href="#前言">前言</a></li>
    <li><a href="#从直觉出发推导gcn">从直觉出发推导GCN</a>
      <ul>
        <li><a href="#layer-wise-propagation">Layer-wise Propagation</a></li>
        <li><a href="#neighborhood-aggregation">Neighborhood Aggregation</a></li>
        <li><a href="#self-connection-and-normalization">Self-connection and Normalization</a></li>
      </ul>
    </li>
    <li><a href="#为什么要对称归一化">为什么要对称归一化</a></li>
    <li><a href="#后记">后记</a></li>
  </ul>
</nav>
</div>
<h2 id="前言">前言</h2>
<p>GCN即Graph Convolutional Network，我个人觉得指代了三种东西，第一种是说GCN这个算法，即Kipf and Welling在ICLR'2017发表的<a href="https://arxiv.org/abs/1609.02907">paper</a>中提出的算法，也是本文介绍的对象；第二种是说基于Spectral Graph Theory的一类图卷积网络，当然Kipf这个模型也是其中一种，此外还有<a href="https://arxiv.org/abs/1606.09375">ChebyNet</a>等；第三种指代最general，主要指那些基于邻居聚合（Neighborhood Aggregation）思想的定义在图上的神经网络模型，但现在大家更倾向用GNNs（Graph Neural Networks）来概括。</p>
<p>*在<a href="https://arxiv.org/abs/1806.01973">PinSAGE</a>模型中，作者还用了这样的脚注来说明文中的GCN是指代第三种：</p>
<blockquote>
<p>Following a number of recent works (e.g., [13, 20]) we use the term &ldquo;convolutional&rdquo; to refer to a module that aggregates information from a local graph region and to denote the fact that parameters are shared between spatially distinct applications of this module; however, the architecture we employ does not directly approximate a spectral graph convolution (though they are intimately related) [6].</p>
</blockquote>
<h2 id="从直觉出发推导gcn">从直觉出发推导GCN</h2>
<p>给定一个无向无权图$\mathcal{G=(V, E)}$，$\mathcal V$是节点的集合，$\mathcal{E\subset V\times V}$是边的集合。$A$是对称的邻接矩阵，$A_{ij}=1\text{ iff }(i, j) \in \mathcal{E}$。注意，图中没有自环则对角元素均为0。</p>
<p>GCN的目的是，给定图$\mathcal{G=(V, E)}$和所有节点的初始特征构成的特征矩阵$H^{(0)} \in \Re^{|\mathcal{V}|\times d_0}$，通过一个神经网络输出各个节点的新的特征$H^{(L)}\in \Re^{|\mathcal{V}|\times d_L}$，特征维度是否相同都无所谓，方便起见，我们假设神经网络每一层的输入输出维数都相同，因此也有$d = d_0 = \cdots = d_L$。</p>
<p>为什么我们输入了节点的特征，最后的输出也还是节点的特征呢？如果将每个节点看作一个实体，那么初始特征可能仅反映了实体自身的特征，而通过图卷积，我们希望利用节点之间的连接关系，丰富其特征表示，这样输出的特征$H^{(L)}\in \Re^{|\mathcal{V}|\times d_L}$应该除了自身的特征外，还“借鉴”了其他节点的特征，这样表达能力会更强。</p>
<h3 id="layer-wise-propagation">Layer-wise Propagation</h3>
<p>神经网络的思想就是学习数据的层次化的特征表示。因此，我们自然地想到应该也可以以一种层次化的方式学习节点的特征，即逐层更新节点的特征：</p>
<p>
$$H^{(l+1)} = f(H^{(l)}, A; W^{(l)}).$$
</p>


<p>$f$是神经网络的一层，拥有参数$W^{(l)}$。我们这里加入了邻接矩阵，是希望特征$H^{(l)}$在传播时，考虑到节点之间的连接信息。那么$f$应该怎么设计呢？这里就要引出最重要的一个思想：Neighborhood Aggregation。</p>
<h3 id="neighborhood-aggregation">Neighborhood Aggregation</h3>
<p>Neighborhood Aggregation的思想其实特别简单，就是一个节点$u$可以由它的邻居来体现$N(u)$，因此，节点的特征就由它的“邻居”的特征“聚合”而成。众多的图卷积算法其实就在“邻居”和“聚合”操作的定义上有所不同。</p>
<p>回到本篇的主题，我们来试着定义一下邻居和聚合操作。最简单的邻居当然就是真的邻居，即与中心节点$u$直接相连的一阶邻居$N(u) = {v | (u, v) \in \mathcal{E} }$，而最简单的聚合就是求和。因此我们有</p>
<p>
$$\tilde{h}_u^{(l+1)} = \sum_{v \in N(u)} h_v^{(l)}. \tag{1}$$
</p>


<p>当然，为了进一步增加表达能力，我们会进行非线性变换：$h_u^{(l+1)} = \sigma(\tilde{h}_u^{(l+1)} \cdot W^{(l)})$，其中$\sigma$是非线性激活函数，$W^{(l)}\in \Re^{d \times d}$是参数矩阵。把这两个式子写成矩阵形式就是</p>
<p>
$$H^{(l+1)} = \sigma\left( AH^{(l)}W^{(l)}\right). \tag{2}$$
</p>


<p>大家可以自己验证一下这个式子。</p>
<h3 id="self-connection-and-normalization">Self-connection and Normalization</h3>
<p>上面讲了Neighborhood Aggregation的思想，得到了一个逐层传播的公式(2)，但这个公式还有两个小问题需要解决一下。</p>
<p>首先，邻接矩阵的对角元素为0，因此我们在应用公式(2)的时候，节点自身的信息会被完全抛弃掉，从式(1)也能看出来，中心节点$u$的新特征与其旧特征完全无关。这可不行，我们得加回来，最直接的方法就是为每个节点加一个自连接（Self-connection），因此我们可以用新的邻接矩阵来代入到上面的公式：$\tilde{A} = A + I$，即原来的邻接矩阵加上对角阵。我们通过简单的加入自连接就解决了自身信息丢失的问题。</p>
<p>另一个问题是，节点的特征是通过对邻居节点的特征相加得到的，这样特征的量级会越来越大，因此需要进行归一化（Normalization）。最直观的方法，我们用均值代替式(1)(2)中的求和就好，写成矩阵形式：</p>
<p>
$$H^{(l+1)} = \sigma\left(\tilde{D}^{-1}\tilde{A}H^{(l)}W^{(l)} \right).      \tag{3}$$
</p>


<p>这里，$\tilde{D}$是由节点的度数

$\tilde{D}_{uu}=\sum_v \tilde{A}_{uv}=\tilde{d}_u$

构成的对角阵，注意，是加入了自连接后的图。你看，如果我们一开始假设邻接矩阵的对角元素为1，就不用每个符号上面多带一个～了。</p>
<p>式(3)确实解决了数量级爆炸的问题，但不够“漂亮”。原本前面乘的$\tilde{A}$还是一个对称阵，结果现在变成了$\tilde{D}^{-1}\tilde{A}$，不对称了，这可不行。因此，我们可以引入对称归一化（symmetric normalization），利用$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$代替。这样，我们就推导出了GCN的逐层传播的公式！
<p>
$$H^{(l+1)} = \sigma\left(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)} \right).      \tag{Kipf and Welling. 2017. (2)}$$
</p>

</p>
<p>现在我们再考察单个节点，来理解一下对称归一化：</p>
<p>
$$\tilde{h}_u^{(l+1)} = \sum_{v\in N(u) \cup \{v\}} \frac{1}{\sqrt{\tilde{d}_u \cdot \tilde{d}_v}} h_v^{(l)}. \tag{4}$$
</p>


<p>其实就是从除以$\tilde{d}_u$变成了除以$\tilde{d}_u$和$\tilde{d}_v$的几何平均。</p>
<p><img src="/imgs/gcn_intuition/pipeline.png" alt="Pipeline"></p>
<p>可以看这个例子来对比一下三种的区别，注意第二和第三个图中加入了Self-connections。图中的箭头只是表明aggregation，并不代表是有向边。</p>
<h2 id="为什么要对称归一化">为什么要对称归一化</h2>
<p>上面我们讲了因为数量级会爆炸，所以要求均值而不是求和，又因为求均值归一化不够漂亮，所以要对称归一化。现在我们直观地来理解一下对称归一化。</p>
<p><img src="/imgs/gcn_intuition/norm.png" alt="Normalization"></p>
<p>我们的中心节点$u$要从一众邻居节点接受信息，如果这个节点有很多邻居，那么他接受到的信息量就很大、很杂，因此，每一个邻居传过来的信息的重要性就会变低。</p>
<p>现在我们换个视角，从信息的发出方$v$来看。节点$v$要把信息传出给它的所有邻居节点，如果它的邻居众多，那它可能就是个发小广告的，因此它传出去的信息的重要性就会变低。</p>
<p>这样，我们看从$u$到$v$的这一条信息，其重要性就要打一定的折扣，$u$觉得这条信息的重要性是原来的$\frac{1}{\tilde{d}_u}$，而$v$觉得是$\frac{1}{\tilde{d}_v}$。为了不打架，我们就取个平均$\frac{1}{\sqrt{\tilde{d}_u \cdot \tilde{d}_v}}$好了😄。</p>
<h2 id="后记">后记</h2>
<p>其实个人觉得，Kipf的博客中介绍的GCN比论文要好太多了，论文强行从图谱理论进行解释，充斥着奇怪的假设与近似，可以参考知乎上的这个<a href="https://zhuanlan.zhihu.com/p/60014316">讨论</a>。但奈何GCN又简洁效果又好，火也是有道理的，而且还推动了整个图神经网络的发展。</p>
<p>这篇文章根据Kipf的博客，从直觉出发对GCN进行了推导。之后有时间，再来为大家介绍一下我了解的图谱理论的一些皮毛。主要想介绍一下热传导方程与图卷积网络的相似之处，真的很有意思。比如热传导方程有Laplace算子，求解要用傅立叶变换；而图卷积里也有Laplace矩阵，推导也涉及到傅立叶变换，这其中究竟隐藏了怎样的秘密？且听下回分解🐦。</p>

		</div>
<script src="https://utteranc.es/client.js"
    repo="yzhang1918/blog_comments"
    issue-term="pathname"
    label="Comment"
    theme="github-light"
    crossorigin="anonymous"
    async>
</script></div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> ©2020 yzhang1918 |  <a href="https://github.com/yzhang1918/ezhil">Theme</a>  Modified from <a href="https://github.com/vividvilla/ezhil">Ezhil</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-77357711-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script>feather.replace()</script>
</body>
</html>
