<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>[GCO系列-前篇01] structure2vec - Ylog</title><link rel="icon" type="image/png" href=icons/Y.png /><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="[GCO系列-前篇01] structure2vec" />
<meta property="og:description" content="解读最早的GNN代表作之一" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yzhang1918.github.io/posts/s2v/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-03-26T21:24:03&#43;08:00" />
<meta property="article:modified_time" content="2020-03-26T21:24:03&#43;08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[GCO系列-前篇01] structure2vec"/>
<meta name="twitter:description" content="解读最早的GNN代表作之一"/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://yzhang1918.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://yzhang1918.github.io/css/main.css" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
	<script src="https://yzhang1918.github.io/js/main.js"></script>
</head>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '\\[', right: '\\]', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false}
                ]
            });"></script>
        

<body>
	<div class="container wrapper post">
		<div class="header">
	<h1 class="site-title"><a href="https://yzhang1918.github.io/">Ylog</a></h1>
	<div class="site-description"><h2>yzhang1918&rsquo;s Blog</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/yzhang1918" title="Github"><i data-feather="github"></i></a><a href="https://www.instagram.com/yaozh1918/" title="Instagram"><i data-feather="instagram"></i></a><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/posts">All posts</a>
			</li>
			
			<li>
				<a href="/categories">Categories</a>
			</li>
			
			<li>
				<a href="/tags">Tags</a>
			</li>
			
			<li>
				<a href="/about">About</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">[GCO系列-前篇01] structure2vec</h1>
			<div class="meta">Posted at &mdash; Mar 26, 2020</div>
		</div>

		<div class="post-tags">
			
				
				<nav class="nav">
						<ul class="flat">
							
							<li class="tags"><a href="/tags/gnn">gnn</a></li>
							
							
							<li class="categories"><a href="/categories/paper-reading">paper-reading</a></li>
							
						</ul>
				</nav>
				
			
		</div>

		<div class="markdown">
			<div class="toc">
<nav id="TableOfContents">
  <ul>
    <li><a href="#pairwise-markov-random-field">Pairwise Markov Random Field</a></li>
    <li><a href="#hilbert-space-embedding-of-distributions">Hilbert Space Embedding of Distributions</a></li>
    <li><a href="#embedding-mean-field-inference">Embedding Mean-Field Inference</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
</div>
<p><code>structure2vec</code>是一个我很喜欢的图神经网络模型，发表于ICML'16。虽然文章引用也不少，但远没有Kipf and Welling的GCN流行，个人认为，主要是<code>structure2vec</code>这篇文章写得唬人，没有GCN那么简洁，另一方面，它比较追求理论，最后的模型参数很少，表达能力可能也不如GCN强。</p>
<p><code>structure2vec</code>其实是一类方法，它将结构化数据建模为pairwise Markov random field，然后找到variational inference过程中的不动点迭代公式，再将其转化为embedding的不动点迭代。所以可以看到<code>structure2vec</code>正文中给出了两种算法，分别对应mean field inference和loopy belief propagation两种VI方法，在附录中又给出了其他四种VI算法下导出的版本。</p>
<h2 id="pairwise-markov-random-field">Pairwise Markov Random Field</h2>
<p>给定一张图$\chi=\mathcal{(V, E)}$, 第$i$个节点上的值用$x_i$表示。文中称$\chi$为一个<em>structured data point</em>，因为这篇文章最后做的是图分类任务，但我们也可以拿来做节点分类任务，因为它是一个通用的节点embedding算法。
我们用随机变量$X_i$对节点$i$建模，同时再关联一个隐变量$H_i$，隐变量之间根据图中的边对应关联。则联合概率分布可以用一个pairwise MRF来表示
<p>
$$
p\left(\left\{H_{i}\right\},\left\{X_{i}\right\}\right) \propto \prod_{i \in \mathcal{V}} \Phi\left(H_{i}, X_{i}\right) \prod_{(i, j) \in \mathcal{E}} \Psi\left(H_{i}, H_{j}\right),
$$
</p>


其中，$\Phi$和$\Psi$为potential。</p>
<p>一般来说，我们写出了图模型的概率分布，下一步就要去学$\Phi$和$\Psi$，但是这并不容易：（1）我们只能观测到$X_i$，隐变量$H_i$让learning很困难；（2）给两个数据点$\chi$和$\chi'$，它们的条件独立性是不同的（也就是图结构不同），这让learning难上加难。</p>
<p>所以，这一篇工作中，作者不走寻常路，直接将$H_i$的后验分布嵌入到一个特征空间中。</p>
<h2 id="hilbert-space-embedding-of-distributions">Hilbert Space Embedding of Distributions</h2>
<p>我们可以将一个概率分布用这样的方式映射到一个特征空间中：
<p>
$$
\mu_{X}:=\mathbb{E}_{X}[\phi(X)]=\int_{\mathcal{X}} \phi(x) p(x) dx,
$$
</p>


这里，$\phi$是一个feature mapping。这样我们就将一个关于$X$的分布映射为了特征空间中的一个点。
举个简单的例子，取$p_t$为$[0, t]$上的均匀分布，feature mapping为$\phi(x) = (x, x^2)^T$。那么
<p>
$$
\mu_t = \int_0^t \frac{1}{t} (x, x^2)^T dx = \left(\frac{t}{2}, \frac{t^2}{3}\right)^T
$$
</p>


为分布$p_t$在二维特征空间上的embedding。</p>
<p>如果$\phi$是单射的，那么embedding也是单射的，这意味如果两个分布不同，那他们的embedding也不同。换句话说，$\mu_{X}$是$p(X)$的充分统计量。
此时，对于任意的关于$p$的泛函或算子，我们总能找到对应的关于其embedding的函数或算子：
<p>
$$
f(p(x))=\tilde{f}\left(\mu_{X}\right), \quad \mathcal{T} \circ p(x)=\widetilde{\mathcal{T}} \circ \mu_{X}.
$$
</p>

</p>
<h2 id="embedding-mean-field-inference">Embedding Mean-Field Inference</h2>
<p>论文正文给了mean-field inference和loopy BP两种inference算法的讨论，我们这里只介绍前者。</p>
<p>Mean-field inference利用一组独立的概率分布去近似真实的后验：

$ p\left(\left\{H_{i}\right\} |\left\{x_{i}\right\}\right) \approx \prod_{i \in \mathcal{V}} q_{i}\left(h_{i}\right).$
</p>
<p>那么怎么去找这一组$q_i$呢？我们通常选择优化他们之间的KL divergence：
<p>
$$\min _{q_{1}, \ldots, q_{d}} \int_{\mathcal{H}^{d}} \prod_{i \in \mathcal{V}} q_{i}\left(h_{i}\right) \log \frac{\prod_{i \in \mathcal{V}} q_{i}\left(h_{i}\right)}{p\left(\left\{h_{i}\right\} |\left\{x_{i}\right\}\right)} \prod_{i \in \mathcal{V}} d h.$$
</p>

</p>
<p>通过一系列推导，充分利用条件独立性，我们可以发现，这个优化问题的解需要满足这个不动点迭代式：
<p>
$$
\log q_i(h_i) =c_{i}^{\prime}+\log \Phi\left(h_{i}, x_{i}\right)+\sum_{j \in \mathcal{N}(i)} \int_{\mathcal{H}} q_{j}\left(h_{j}\right) \log \Psi\left(h_{i}, h_{j}\right) d h_{j},
$$
</p>


其中$c$是个常量，$\mathcal{N}(i)$就是邻居节点集合了。
这个不动点迭代式说明$q_{i}$是它邻居的一个泛函：
<p>
$$
q_{i}\left(h_{i}\right)=f\left(h_{i}, x_{i},\left\{q_{j}\right\}_{j \in \mathcal{N}(i)}\right).
$$
</p>


是不是有neighbor aggregation内味儿了？</p>
<p>我们现在运用之前讨论分布embedding的结论，如果$q_i$有单射embedding
<p>
$$\widetilde{\mu}_{i}=\int_{\mathcal{H}} \phi\left(h_{i}\right) q_{i}\left(h_{i}\right) d h_{i},$$
</p>


那么不动点迭代也能写成关于embedding的形式：
<p>
$$
q_{i}\left(h_{i}\right)=\tilde{f}\left(h_{i}, x_{i},\left\{\widetilde{\mu}_{j}\right\}_{j \in \mathcal{N}(i)}\right).
$$
</p>


我们进一步把左边也写成关于embedding的形式：
<p>
$$
\widetilde{\mu}_{i}=\widetilde{\mathcal{T}} \circ\left(x_{i},\left\{\widetilde{\mu}_{j}\right\}_{j \in \mathcal{N}(i)}\right).
$$
</p>


最后，我们再将算子 $\widetilde{\mathcal{T}}$参数化，就得到了直接关于embedding的不动点迭代公式，也就是<code>structure2vec</code>的逐层传播公式：
<p>
$$
\widetilde{\mu}_{i}=\sigma\left(W_{1} x_{i}+W_{2} \sum_{j \in \mathcal{N}(i)} \tilde{\mu}_{j}\right).
$$
</p>

</p>
<p>按照正常的流程，我们应该先去学习$\Phi$和$\Psi$，然后再找$\widetilde{\mathcal{T}}$，同时别忘了还要确定feature mapping $\phi$。但这篇文章另辟蹊径，直接参数化$\widetilde{\mathcal{T}}$，再用最后的任务（这里是图分类）的监督信号来学习参数。</p>
<p>最后的算法是这样的：
<img src="/imgs/structure2vec/algo.png" alt="Algorithm">
注意，$T$可以理解为图神经网络的深度，但这里其实是计算不动点的迭代次数；另一方面，迭代过程中，权重矩阵始终不变，所以这个模型的参数量是非常少的。</p>
<h2 id="conclusion">Conclusion</h2>
<p>我们用这篇短短的blog简单回顾了下<code>structure2vec</code>这个经典的GNN模型。虽然它也能被归类在message passing框架下，但可以发现它和现在主流的spectral/spatial based GNN走了截然不同的推导路径。
它的核心思想是不动点迭代，以后有机会可以讲讲这组作者的另一篇paper，SSE，同样是走的不动点迭代的推导路径。（同时SSE中的对比算法有GCN和<code>structure2vec</code>，可以发现<code>structure2vec</code>略逊与GCN。）</p>
<p>那么在下一篇blog中，我将为大家讲解用GNN+RL做图组合优化的经典paper：<code>S2V-DQN</code>。</p>

		</div>
<script src="https://utteranc.es/client.js"
    repo="yzhang1918/blog_comments"
    issue-term="pathname"
    label="Comment"
    theme="github-light"
    crossorigin="anonymous"
    async>
</script></div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> ©2020 yzhang1918 |  <a href="https://github.com/yzhang1918/ezhil">Theme</a>  Modified from <a href="https://github.com/vividvilla/ezhil">Ezhil</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-77357711-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script>feather.replace()</script>
</body>
</html>
