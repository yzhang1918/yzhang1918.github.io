<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>reinforcement-learning on Ylog</title>
    <link>https://yzhang1918.github.io/tags/reinforcement-learning/</link>
    <description>Recent content in reinforcement-learning on Ylog</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>©2020 yzhang1918</copyright>
    <lastBuildDate>Wed, 08 Apr 2020 11:36:05 +0800</lastBuildDate><atom:link href="https://yzhang1918.github.io/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[GCO系列-番外01] ClusterNet</title>
      <link>https://yzhang1918.github.io/posts/clusternet/</link>
      <pubDate>Wed, 08 Apr 2020 11:36:05 +0800</pubDate>
      
      <guid>https://yzhang1918.github.io/posts/clusternet/</guid>
      <description>Motivation Related Work ClusterNet Forward Pass Backward Pass A Visual Proof Solving the Optimization Problems Experimental Results Conclusion Wilder et al., End to End Learning and Optimization on Graphs. NeurIPS&#39;2019. 这篇是我在遍历Graph Combinatorial Optimization相关工作过程中发现的一篇pap</description>
    </item>
    
    <item>
      <title>Soft Policy Iteration</title>
      <link>https://yzhang1918.github.io/posts/sac/</link>
      <pubDate>Thu, 02 Jan 2020 20:17:15 +0800</pubDate>
      
      <guid>https://yzhang1918.github.io/posts/sac/</guid>
      <description>This is a note on the soft policy iteration from SAC12.
 Soft Policy Evaluation Soft Policy Improvement Soft Policy Iteration Other References    Soft Policy Evaluation We define the bellman backup operator for any $Q: \mathcal{S\times A} \rightarrow \Re$:
 $$ \mathcal{T}^\pi Q(s_t, a_t) \triangleq r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p} [V(s_{t+1})] $$ where we have the soft state value function:
$$V(s_t) = \mathbb{E}_{a_t\sim \pi}[Q(s_t, a_t) - \alpha \log\pi(a_t|s_t)] $$</description>
    </item>
    
  </channel>
</rss>
